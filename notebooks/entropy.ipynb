{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's an intuitive way to think of cross entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [What's an intuitive way to think of cross entropy](https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Information Entropy\n",
    "\n",
    "**Source:**\n",
    "\n",
    ">- [A Gentle Introduction to Information Entropy](https://machinelearningmastery.com/what-is-information-entropy/)\n",
    "\n",
    "## What is information theory\n",
    "\n",
    "Information theory is field of study concerned with quantifying information for communication.\n",
    "\n",
    "*信息论是涉及量化沟通信息的研究领域.*\n",
    "\n",
    "A foundational concept from information is the quantification of the amount of information in things like events, random variables, and distribution.\n",
    "\n",
    "*信息的一个基本概念是对诸如事件, 随机变量和分布的信息量的量化.*\n",
    "\n",
    "> *Why unify information theory and machine learning? Because they are two side of the ame coin. Information theory and machine learning still alone together.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the information for an event\n",
    "\n",
    "Quantifying information is the foundation of field of information theory.\n",
    "\n",
    "The intuition behind quantifying information is the idea of measuring how much surprise there is in an event. Those events that rare (low probability) are more surprising and therefore have more information those events that there are common (high probability).\n",
    "\n",
    "- **Low probability event:** High information (surprising).\n",
    "- **High probability event:** Low information (unsurprising).\n",
    "\n",
    "Rare events are more uncertain or more surprising and require more information to represent them than common events.\n",
    "\n",
    "We can calculate the amount of information there is in an event using the probability of the event. This is called *\"Shannon information\", \"self-information\"*, or simply the *\"information\"*, and can be calculated for a discrete event $x$ as follows:\n",
    "\n",
    "- $information(x) = -\\log_2(p(x))$\n",
    "\n",
    "Where $p(x)$ is the probability of the event $x$.\n",
    "\n",
    "The choice of the 2-base logarithm means that the units of information measure is in bits(binary digits).\n",
    "\n",
    "The calculate of information is often written as $h()$ for example:\n",
    "\n",
    "- $h(x) = -\\log_2(p(x))$\n",
    "\n",
    "The negative sign ensures that the result is always positive or zero.\n",
    "\n",
    "Information will be zero when the probability of an event is $1.0$ or a certainty, e.g. there is no surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this concrete with some examples.\n",
    "\n",
    "Consider a flip of a single fair coin. The probability of heads(and tails) is 0.5. We can calculate the information for flipping a head in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:20:12.817600Z",
     "start_time": "2019-11-12T03:20:12.813600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T03:36:42.021001Z",
     "start_time": "2019-11-12T03:36:41.976001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x)=0.5, information: 1.0\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "h = -np.log2(p)\n",
    "print(f'p(x)={p}, information: {h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Entropy for a Random Variable\n",
    "\n",
    "$\n",
    "H(X)=-\\displaystyle\\sum_{k \\in K} p_k \\log p_k\n",
    "$\n",
    "\n",
    "上面的公式含义是, $H(X)$ 表示随机变量 $X$ 的熵, 它等于随机变量 $X$ 的 $K$ 种状态下, 每种状态 $k$ 的概率 $p_k$ 乘以 每种状态 $k$ 的概率的对数.\n",
    "\n",
    "The lowest entropy is calculated for random variable that has a single event with a probability of 1.0, a certainly.\n",
    "\n",
    "*随机变量熵的最小值为 $0$, 也就是当随机变量只有一个事件, 换句话说就是该随机变量的概率等于 $1$. 带入公式很容易得到熵的结果为 $0$*\n",
    "\n",
    "The largest entropy for a random variable will be if all events are equally likely.\n",
    "\n",
    "*当随机变量中的所有事件的概率相等(均匀分布), 此时该随机变量的熵值最大.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Cross-Entropy for Machine Learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to entropy, cross entropy and KL divergence in machine learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - [An introduction to entropy, cross entropy and KL divergence in machine learning](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gain and Mutual Information for Machine Learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - (Information Gain and Mutual Information for Machine Learning)[https://machinelearningmastery.com/information-gain-and-mutual-information/]\n",
    "\n",
    "$\n",
    "I(X;Y) = H(Y) - H(Y|X)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Calculate the KL Divergence for Machine Learning\n",
    "\n",
    "**Source:**\n",
    "\n",
    "> - (How to Calculate the KL Divergence for Machine Learning)[https://machinelearningmastery.com/divergence-between-probability-distributions/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "568px",
    "left": "301px",
    "top": "143px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
